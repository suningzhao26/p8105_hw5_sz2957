---
title: "Homework 5"
author: "Suning Zhao"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(purrr)
library(rvest)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))

set.seed(1)
```


## Problem 1

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r, message=FALSE, warning=FALSE}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

## Problem 2

### Raw Data Import and Description

* The code chunk below imports the data from github website. To do this, I create a dataframe using `read_csv` to read the csv file. Then, I add a new variable `city_state` to illustrate the city and state together.
* I notice that in one sample, Tusla has been wrongly allocated to AL instead of OK. I have solved that by `str_replace`

```{r, message=FALSE, warning=FALSE}
homicides_raw_df = 
  read_csv("data/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city,state,sep=",") %>% 
    str_replace("Tulsa,AL", "Tulsa,OK")
  ) 
```

* The raw data includes `r ncol(homicides_raw_df)` columns and `r nrow(homicides_raw_df)`. 
* Important variables include `r colnames(homicides_raw_df)`, which are used to describe the homicide in 50 cities across US.

### Summarize the total number of homicides and unsolved homicides

* The code chunk below generates a table to summarize the total number of homicides and unsolved homicides in each cities.

```{r}
homicides_count_df = 
  homicides_raw_df %>% 
  mutate(
    homicides_status = ifelse(disposition != "Closed by arrest", "unsolved", "solved")
    ) %>% 
  group_by(city_state) %>% 
  summarise(n_unsolved = sum(homicides_status == "unsolved"),
            n_total = n())

homicides_count_df %>% 
  knitr::kable(digits = 3) 
```

### Estimate proprotion of homicides that are unsloved for Baltimore, MD

* The code chunk below use `prop.test` function and `broom::tidy` to estimate the proportion of homicides that are unsolved in Baltimore, MD.

```{r}
baltimore_count_df = 
  homicides_count_df %>% 
  filter(city_state == "Baltimore,MD")

prop_test = 
  prop.test(
    x = baltimore_count_df %>% pull(n_unsolved), 
    n = baltimore_count_df %>% pull(n_total)) 

baltimore_prop_df = 
  broom::tidy(prop_test) %>% 
  rename(
    lower_limit = conf.low,
    upper_limit = conf.high
  )

baltimore_prop_df %>% 
  select(estimate, lower_limit, upper_limit) %>% 
  knitr::kable(digits = 3)
```

* From the result we can know that the estimated proportion of homicides that are unsolved in Baltimore is 0.646, with a confidence interval between 0.628 and 0.663

### Estimate proprotion of homicides that are unsloved for all cities

* The code chunk below use `prop.test` function, `map2` and `broom::tidy` to estimate the proportion of homicides that are unsovled in all cities.

```{r}
homicides_test_df = 
  homicides_count_df %>%
  mutate(
      prop_test = map2(n_unsolved, n_total, ~ prop.test(.x, .y) %>%
      broom::tidy())) %>%
  unnest(prop_test) %>%
  rename(
    lower_limit = conf.low,
    upper_limit = conf.high
  ) %>% 
  select(city_state, estimate, lower_limit, upper_limit)

homicides_test_df %>% 
  knitr::kable(digits = 3)
```

### Creat a plot to show the estimates and CIs for each city

* The code chunk below use `ggplot` to generate a plot to show the estimated proportion of unsolved homicides for each city.
* We used `geom_errorbar` to create the plot. The cities are organized accroding to the proportion of unsolved homicides.

```{r}
homicides_test_df %>%
  mutate(
    city_state = fct_reorder(city_state, estimate, .desc = TRUE)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower_limit, ymax = upper_limit, width = .3)) +
  theme(axis.text.x = element_text(angle = 75, hjust = 1), legend.position = "none") +
  labs(
    title = "Proportion of unsolved homicides in large cities in the US",
    x = "Location (City/State)",
    y = "Proportion of unsolved homicides"
  )
```


## Problem 3

### Set design elements:

According to the requirement, build a function for t-test, based on a random selected dataset from normal distribution that fix n = 30 and sigma = 5

```{r}
t_test = function(n = 30, mu, sigma = 5) {
  simulation_data = tibble(
    x = rnorm(n = 30, mean = mu, sd = sigma)
  )
  simulation_data %>% 
    t.test(mu = 0, conf.level = 0.95, alternative = c("two.sided")) %>%
    broom::tidy() %>% 
    select(estimate, p.value)
}
```

### Test when u = 0. Generate 5000 datasets.

```{r}
simulation_df_0 = 
  expand_grid(
    mu = 0,
    iterate = 1:5000) %>% 
  mutate(
    t_test_df = map(.x = mu, ~t_test(mu = .x))
    ) %>%
  unnest(t_test_df)
```

### Repeat test for u = 1:6

```{r}
simulation_df = 
  expand_grid(
    mu = c(1:6),
    iterate = 1:5000) %>% 
  mutate(
    t_test_df = map(.x = mu, ~t_test(mu = .x))
    ) %>%
  unnest(t_test_df) %>% 
  rename(p_value = p.value)
```

### Making Plots showing the proportion of times the null was rejected

```{r}
simulation_df = 
 simulation_df %>% 
  mutate(
    reject = if_else(p_value < 0.05, "Reject", "Fail")) 

simulation_df %>% 
  group_by(mu, reject) %>%
  summarize(
    num_reject = n()) %>%
  mutate(
    prop_reject = num_reject/5000) %>% 
  filter(reject == "Reject") %>%
  ggplot(
    aes(x = mu, y = prop_reject)) +
  geom_point() + 
  geom_line() + 
  labs(
    title = "Association between effect size and power of the t-test",
    x = "True value of mean",
    y = "Proportion of times the null was rejected"
  )
```

* Based on the result, we can know that as the true mean increases, the proportion of times the null was rejected also increases. It is approaching to 1 when the true mean is closing to 6.
* Since the power of the test is the proportion of times the null was rejected and the effect size is the difference between true mean and 0, we can know that the larger the effect size is, the higher the power of test is. 

### Make a plot showing the average estimate of mu on the y axis and the true value of u on the x axis
```{r}
average_total = 
  simulation_df %>%
  group_by(mu) %>% 
  summarize(
    estimate_avg = mean(estimate)
    ) %>% 
  mutate(data = "All")

average_reject = 
  simulation_df %>%
  filter(reject == "Reject") %>% 
  group_by(mu) %>% 
  summarize(
    estimate_avg = mean(estimate)
    ) %>% 
  mutate(data = "Reject")

simulation_average = bind_rows(average_total, average_reject)

simulation_average %>% 
ggplot(aes(x = mu, y = estimate_avg, color = data)) +
  geom_point() + 
  geom_path() +
  scale_x_continuous( breaks = 1:6 ) +
  scale_y_continuous( breaks = 1:6 ) +
  labs(
    title = "Average estimate of mu vs. True mu",
    x = "True mu",
    y = "Average estimate of mu"
  ) + 
  theme(legend.position = "bottom")
```

* According to the graph, we can know that when mu is less than 3, the sample average of mu across tests for which the null is rejected is higher than true value of mu.
* When mu is higher than 4, it is suggested that the sample average of mu across tests for which the null is rejected is approximately equal to the true value of mu.
* From last graph we can know that the power is relatively lower when mu is less than 3. That is the reason why the sample average is higher than the true value of mu when mu is less than 3. Under a lower power, the estimates of mu is not precise, which we should pay attention in statistical analysis.
